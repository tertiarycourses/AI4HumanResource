{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI for HR - 10 Jun 2021.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaHmq26gtnC_"
      },
      "source": [
        "# Use Case 1- Predict Employee Attrition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCfjS2e_tyqU"
      },
      "source": [
        "## Preprocessing Attrition Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoJAf1lOt1n3"
      },
      "source": [
        "#Load the dataset and analyze\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "attrition_data = pd.read_csv(\"employee_attrition.csv\")\n",
        "\n",
        "print(\"Data Loaded:\\n------------------------\\n\",attrition_data.dtypes)\n",
        "attrition_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_bPUVCJt7HQ"
      },
      "source": [
        "#Correlation Analysis of target attribute\n",
        "\n",
        "attrition_data.corr()['Attrition']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0rRAmA0uA3r"
      },
      "source": [
        "#Convert to Dataframe to numpy array\n",
        "np_attrition = attrition_data.to_numpy().astype(float)\n",
        "\n",
        "#Create X_train with the first 7 attributes\n",
        "X_train = np_attrition[:,1:7]\n",
        "#Create Y_train with attrition attribute\n",
        "Y_train=np_attrition[:,7]\n",
        "\n",
        "#Convert Y_train to one-hot-encoding\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train,2)\n",
        "\n",
        "print(\"X-Train Shape : \", X_train.shape)\n",
        "print(\"Y-Train Shape : \", Y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gteKNSm8uHHh"
      },
      "source": [
        "## Build Attrition model with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_Kq6GRJuImN"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "#Setup hyperparameters for deep learning\n",
        "EPOCHS=100\n",
        "BATCH_SIZE=100\n",
        "VERBOSE=1\n",
        "NB_CLASSES=2\n",
        "N_HIDDEN=128\n",
        "VALIDATION_SPLIT=0.2\n",
        "\n",
        "#Create a Keras model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Add first hidden Dense layer\n",
        "model.add(keras.layers.Dense(N_HIDDEN,\n",
        "                             input_shape=(6,),\n",
        "                              name='Dense-Layer-1',\n",
        "                              activation='relu'))\n",
        "\n",
        "#Add a second hidden dense layer\n",
        "model.add(keras.layers.Dense(N_HIDDEN,\n",
        "                              name='Dense-Layer-2',\n",
        "                              activation='relu'))\n",
        "\n",
        "#Add a final layer with softmax\n",
        "model.add(keras.layers.Dense(NB_CLASSES,\n",
        "                             name='Final',\n",
        "                             activation='softmax'))\n",
        "\n",
        "#Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Fit parameters \n",
        "model.fit(X_train,\n",
        "          Y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=VERBOSE,\n",
        "          validation_split=VALIDATION_SPLIT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzq9DozvuS0b"
      },
      "source": [
        "## Predict Attrition with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1Hu253buU_9"
      },
      "source": [
        "TotalMonthsOfExp=40\n",
        "TotalOrgsWorked=4\n",
        "MonthsInOrg=20\n",
        "LastPayIncrementBand=5\n",
        "AverageFeedback=4\n",
        "LastPromotionYears=4\n",
        "\n",
        "print(\"Will employee leave ?\", model.predict_classes([[TotalMonthsOfExp,\n",
        "                                  TotalOrgsWorked,\n",
        "                                  MonthsInOrg,\n",
        "                                  LastPayIncrementBand,\n",
        "                                  AverageFeedback,\n",
        "                                  LastPromotionYears]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcqqSTEVuatZ"
      },
      "source": [
        "#Bulk predictions\n",
        "\n",
        "print(model.predict_classes(\n",
        "    [[111,5,85,3,2,2],\n",
        "    [31,2,15,4,1,4],\n",
        "    [61,4,24,1,4,3],\n",
        "    [77,4,35,3,1,1],\n",
        "    [81,5,7,1,2,3],\n",
        "    [113,4,112,5,4,1],\n",
        "    [101,2,48,5,1,4],\n",
        "    [45,4,22,5,3,1],\n",
        "    [25,2,2,2,3,2],\n",
        "    [97,3,15,3,2,4]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daTTkl2HyOLa"
      },
      "source": [
        "# Use Case 2 - Discovering Virtual Teams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9DglgJbybNx"
      },
      "source": [
        "## Preparing Network Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-URqe5jyWkh"
      },
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from csv import reader\n",
        "import pandas as pd\n",
        "\n",
        "#Input file with one record per chat collaboration\n",
        "chat_csv = \"chat_groups.csv\"\n",
        "\n",
        "#Data frame to store employee pairs.\n",
        "employee_pairs = pd.DataFrame(columns=['First', 'Second', 'Count'])\n",
        "\n",
        "#Read file and extract pairs and weights\n",
        "with open(chat_csv, 'r', encoding=\"utf-8-sig\") as read_obj:\n",
        "    # pass the file object to reader() to get the reader object\n",
        "    csv_reader = reader(read_obj)\n",
        "    # Iterate over each row in the csv using reader object\n",
        "    for row in csv_reader:\n",
        "        #Sort by employee name\n",
        "        row.sort()\n",
        "        #sort and filter for only valid names\n",
        "        filtered_row = [ emp for emp in row\n",
        "                        if len(emp) > 0] \n",
        "\n",
        "        #Generate employee pairs\n",
        "        \n",
        "        #Iterate for the first employee\n",
        "        for i in range(0, len(filtered_row)-1):\n",
        "            #Iterate for the second employee\n",
        "            for j in range(i+1,len(filtered_row) ):\n",
        "            \n",
        "                first=filtered_row[i]\n",
        "                second=filtered_row[j]\n",
        "\n",
        "                #Create the pair record. If Dataframe record already exists\n",
        "                #Update the count. If not, create it\n",
        "                curr_rec = employee_pairs[\n",
        "                                (employee_pairs['First'] == first )\n",
        "                                & (employee_pairs['Second'] == second)]\n",
        "\n",
        "                if ( curr_rec.empty) :\n",
        "                    new_df = pd.DataFrame([{'First': first,\n",
        "                                            'Second' : second,\n",
        "                                            'Count':1}])\n",
        "                    employee_pairs=employee_pairs.append(new_df,\n",
        "                                                         ignore_index=True)\n",
        "\n",
        "                else:\n",
        "                    curr_rec.at[curr_rec.index[0],'Count'] = curr_rec.at[curr_rec.index[0],'Count'] + 1\n",
        "                    employee_pairs.update(curr_rec)\n",
        "                \n",
        "print(employee_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIH2mQ2-yfvx"
      },
      "source": [
        "## Create and visualize the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz1-LYqRyWsi"
      },
      "source": [
        "#Create a networkX graph\n",
        "graph_emps  = nx.Graph()\n",
        "\n",
        "#Add Edges based on the dataframe (nodes gets added automatically)\n",
        "for i,row in employee_pairs.iterrows():\n",
        "    graph_emps.add_edge(row['First'],  \n",
        "                        row['Second'],   \n",
        "                        weight=row['Count'])\n",
        "\n",
        "\n",
        "#Print network summary\n",
        "print(\"Network summary: \\n-----------------\\n\", nx.info(graph_emps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4mLrmP1ym2q"
      },
      "source": [
        "# Create different types of edges based on their cohesion\n",
        "\n",
        "#Pairs with Count > 5 for high cohesion\n",
        "elarge = [(x1, x2) for (x1, x2, data) in graph_emps.edges(data=True) \n",
        "          if data['weight'] > 5]\n",
        "\n",
        "#Pairs with Count between 4 and 5 for medium cohesion\n",
        "emedium = [(x1, x2) for (x1, x2, data) in graph_emps.edges(data=True) \n",
        "          if  3 < data['weight'] <= 5]\n",
        "\n",
        "#Pairs with Count less than 4 for low cohesion\n",
        "esmall = [(x1, x2) for (x1, x2, data) in graph_emps.edges(data=True) \n",
        "          if data['weight'] <= 3]\n",
        "\n",
        "pos = nx.spring_layout(graph_emps)  # positions for all nodes\n",
        "\n",
        "## Setup the Graph\n",
        "# nodes\n",
        "nx.draw_networkx_nodes(graph_emps, pos, \n",
        "                       node_size=700,\n",
        "                       node_color='orange')\n",
        "\n",
        "\n",
        "nx.draw_networkx_edges(graph_emps, pos, \n",
        "                       edgelist=elarge,\n",
        "                       width=6,\n",
        "                       edge_color='blue')\n",
        "\n",
        "nx.draw_networkx_edges(graph_emps, pos, \n",
        "                       edgelist=emedium,\n",
        "                       width=4,\n",
        "                       edge_color='green')\n",
        "\n",
        "nx.draw_networkx_edges(graph_emps, pos, \n",
        "                       edgelist=esmall,\n",
        "                       width=2, \n",
        "                       edge_color='gray')\n",
        "\n",
        "# labels\n",
        "nx.draw_networkx_labels(graph_emps, \n",
        "                        pos, \n",
        "                        font_size=16, \n",
        "                        font_family='Consolas')\n",
        "\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1wjiFVb0Fsa"
      },
      "source": [
        "## Analyzing the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4lG8lPk0Jky"
      },
      "source": [
        "#Function to sort a dictionary by value\n",
        "def sort_dict(dict):\n",
        "    sorted_dict= sorted(dict.items(), key=lambda x: x[1],reverse=True)\n",
        "    \n",
        "    for key,value in sorted_dict:\n",
        "        print(key, \" = \", value)\n",
        "\n",
        "\n",
        "#find number of nodes they are connected with\n",
        "print(\"\\nNodes Mason is connected with :\\n-------------------------------\")\n",
        "print(nx.degree(graph_emps,'Mason'))\n",
        "\n",
        "#clustering - how close a team they form\n",
        "print(\"\\nClustering Co-efficient:\\n----------------------\")\n",
        "sort_dict(nx.clustering(graph_emps,weight='weight'))\n",
        "\n",
        "#Find centrality of nodes\n",
        "print(\"\\nCentrality :\\n---------------\")\n",
        "sort_dict(nx.degree_centrality(graph_emps))\n",
        "\n",
        "print(\"\\nBetweenness:\\n--------------\")\n",
        "sort_dict(nx.betweenness_centrality(graph_emps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUJNdO-Fk-Ic"
      },
      "source": [
        "# Use Case 3 - Recommend Courses to Employees\n",
        "\n",
        "This exercise builds a model that predicts the rating, a given employee will provide for a given course. We then use this model to identify courses that the employee would prefer most"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urE-xcTGlOT6"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXdFmltLkQ3F"
      },
      "source": [
        "#Loading data\n",
        "\n",
        "from csv import reader\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "ratings_data = pd.read_csv(\"employee_course_ratings.csv\")\n",
        "\n",
        "ratings_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT-CIBzclQ3B"
      },
      "source": [
        "## Prepare for Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgvMJMhqklD6"
      },
      "source": [
        "#Build list of unique Employees\n",
        "emp_list=ratings_data.groupby(\n",
        "    ['EmployeeID','EmpName']).size().reset_index()\n",
        "emp_list.head()\n",
        "print(\"Total Employees: \",len(emp_list))\n",
        "\n",
        "#Build list of unique Courses\n",
        "course_list=ratings_data.groupby(\n",
        "    ['CourseID','CourseName']).size().reset_index()\n",
        "course_list.head()\n",
        "\n",
        "print(\"Total Courses: \", len(course_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWTj0oulkoGa"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "#build employee embedding vector\n",
        "#we are using IDs a the direct index to embedding.\n",
        "#Since IDs are continuous, we dont need ID-name mapping.\n",
        "#We can also build a vocabulary alternatively.\n",
        "\n",
        "emp_input = Input(shape=[1], name=\"Emp-Input\")\n",
        "emp_embed = Embedding(2001,  #max value of employee ID\n",
        "                      5,\n",
        "                      name=\"Emp-Embedding\")(emp_input)\n",
        "emp_vec = Flatten(name=\"Emp-Flatten\")(emp_embed)\n",
        "\n",
        "#build course embedding vector\n",
        "course_input = Input(shape=[1],name=\"Course-Input\")\n",
        "course_embed = Embedding(len(course_list) + 1,\n",
        "                         5,\n",
        "                         name=\"Course-Embedding\")(course_input)\n",
        "course_vec = Flatten(name=\"Course-Flatten\")(course_embed)\n",
        "\n",
        "#merge the vectors\n",
        "merged_vec = Concatenate()([emp_vec,course_vec])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjwfwWyvlaDS"
      },
      "source": [
        "## Building the Keras Rating Model\n",
        "\n",
        "The recommendation works as follows\n",
        "\n",
        "1. Build a model that can predict the rating, a given employee may give to a course he/she has not taken so far\n",
        "2. Use the model to predict possible ratings for all courses, for this employee.\n",
        "3. Recommend the courses that have the top predicted ratings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBVK9EAGkt3i"
      },
      "source": [
        "#Given an Employee and a Course, this model predicts the \n",
        "#rating the employee will give this couse\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ratings_train, ratings_test = train_test_split(\n",
        "                                ratings_data, test_size=0.1)\n",
        "\n",
        "#add fully connected layers\n",
        "fc_layer1 = Dense(128,activation=\"relu\")(merged_vec)\n",
        "fc_layer2 = Dense(32, activation=\"relu\")(fc_layer1)\n",
        "model_output = Dense(1)(fc_layer2)\n",
        "\n",
        "rating_model= Model([emp_input,course_input],model_output)\n",
        "\n",
        "rating_model.compile(optimizer=\"adam\",\n",
        "                     loss=\"mean_squared_error\")\n",
        "\n",
        "rating_model.summary()\n",
        "\n",
        "print(\"Fitting the model:\")\n",
        "#Fit the model\n",
        "model_fit = rating_model.fit(\n",
        "    x=[ratings_train.EmployeeID, ratings_train.CourseID],\n",
        "    y=ratings_train.Rating,\n",
        "    epochs=25,\n",
        "    verbose=1,\n",
        "    validation_split=0.1\n",
        "    )\n",
        "\n",
        "print(\"Evaluating the model:\")\n",
        "rating_model.evaluate(\n",
        "    x=[ratings_test.EmployeeID, ratings_test.CourseID],\n",
        "    y=ratings_test.Rating)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kAagmcxlnr9"
      },
      "source": [
        "## Recommending Courses with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVn0LISnleaA"
      },
      "source": [
        "#Predicting the Rating for a given employee and a course\n",
        "#for employee 1029 and course 8\n",
        "\n",
        "rating_model.predict(\n",
        "    [pd.Series([1029]),\n",
        "     pd.Series([8])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDbSDW9Nlryg"
      },
      "source": [
        "emp_to_predict=\"Harriot Laflin\"\n",
        "\n",
        "#Get employee ID for the employee name\n",
        "pred_emp_id=emp_list[emp_list['EmpName'] == emp_to_predict][\"EmployeeID\"].iloc[0]\n",
        "\n",
        "#find Courses already taken by employee. We dont want to predict those.\n",
        "completed_courses=ratings_data[\n",
        "                    ratings_data[\"EmployeeID\"] == pred_emp_id][\"CourseID\"].unique()\n",
        "\n",
        "#Courses not taken by employee\n",
        "new_courses = course_list.query(\"CourseID not in @completed_courses\")[\"CourseID\"]\n",
        "\n",
        "#Create a list with the same employee ID repeated for the same number of times as the\n",
        "#number of new courses. This provides the employee and course Series with same size\n",
        "emp_dummy_list=pd.Series(np.array([pred_emp_id for i in range(len(new_courses))]))\n",
        "\n",
        "#Predict ratings for the new courses for this employee\n",
        "projected_ratings = rating_model.predict([emp_dummy_list,new_courses])\n",
        "flat_ratings = np.array([x[0] for x in projected_ratings])\n",
        "\n",
        "print(\"Course Ratings: \", flat_ratings)\n",
        "\n",
        "#Recommend top 5 courses\n",
        "print(\"\\nRating  CourseID CourseName\\n-----------------------------------\")\n",
        "for idx in (-flat_ratings).argsort()[:5]:\n",
        "    course_id=new_courses.iloc[idx]\n",
        "    course_name=course_list.query(\"CourseID == @course_id\")[\"CourseName\"].iloc[0]\n",
        "    print(\" \", round(flat_ratings[idx],1),\"    \", course_id, \"   \", course_name)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P-5E77Gl3N0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}